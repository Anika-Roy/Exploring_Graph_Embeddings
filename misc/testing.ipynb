{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "print(cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:10:32: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=5, alpha=0.03>', 'datetime': '2023-10-24T18:10:32.295516', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-87-generic-x86_64-with-glibc2.31', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=0,\n",
    "                     window=3,      # context window size\n",
    "                     vector_size=5, # embedding dimension\n",
    "                     sample=5e-5,   # threshold for configuring which higher-frequency words are randomly downsampled (apparently highly important)\n",
    "                     alpha=0.03, \n",
    "                #      min_alpha=0.0007, \n",
    "                     sg=1,          # 1 for skip-gram; otherwise CBOW\n",
    "                     negative=5,    # for negative sampling\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['food', 'is', 'an', 'integral', 'part', 'of', 'human', 'existence', 'sustaining', 'life', 'nourishing', 'our', 'bodies', 'and', 'delighting', 'our', 'senses'], ['fruits', 'and', 'vegetables', 'are', \"nature's\", 'bounty', 'offering', 'an', 'array', 'of', 'flavors', 'textures', 'and', 'colors'], ['protein-rich', 'foods', 'like', 'meat', 'fish', 'eggs', 'and', 'legumes', 'are', 'essential', 'for', 'growth', 'repair', 'and', 'overall', 'health'], ['dairy', 'products', 'including', 'milk', 'cheese', 'and', 'yogurt', 'are', 'valued', 'for', 'their', 'calcium', 'content', 'and', 'contribution', 'to', 'strong', 'bones', 'and', 'teeth'], ['spices', 'and', 'herbs', 'add', 'depth', 'and', 'complexity', 'to', 'dishes'], ['street', 'food', 'is', 'an', 'essential', 'part', 'of', 'culinary', 'exploration', 'allowing', 'us', 'to', 'taste', 'the', 'essence', 'of', 'local', 'culture'], ['exploring', 'exotic', 'and', 'unusual', 'foods', 'is', 'an', 'adventure', 'in', 'itself'], ['food', 'is', 'not', 'merely', 'sustenance', 'it', 'is', 'a', 'reflection', 'of', 'our', 'culture', 'history', 'and', 'lifestyle'], ['climate', 'change', 'is', 'one', 'of', 'the', 'most', 'pressing', 'issues', 'facing', 'humanity', 'today'], ['at', 'the', 'heart', 'of', 'climate', 'change', 'are', 'the', 'emissions', 'of', 'greenhouse', 'gases', 'primarily', 'carbon', 'dioxide', '(co2)', 'methane', '(ch4)', 'and', 'nitrous', 'oxide', '(n2o)'], ['the', 'most', 'evident', 'consequence', 'of', 'climate', 'change', 'is', 'the', 'increase', 'in', 'global', 'temperatures'], ['as', 'temperatures', 'rise', 'polar', 'ice', 'caps', 'and', 'glaciers', 'are', 'melting', 'at', 'an', 'alarming', 'rate'], ['climate', 'change', 'is', 'responsible', 'for', 'more', 'frequent', 'and', 'severe', 'weather', 'events'], ['increased', 'co2', 'levels', 'in', 'the', 'atmosphere', 'also', 'lead', 'to', 'higher', 'levels', 'of', 'carbonic', 'acid', 'in', 'the', 'oceans', 'causing', 'ocean', 'acidification'], ['climate', 'change', 'contributes', 'to', 'habitat', 'destruction', 'and', 'disruption', 'leading', 'to', 'the', 'extinction', 'of', 'many', 'plant', 'and', 'animal', 'species'], ['climate', 'change', 'disproportionately', 'affects', 'vulnerable', 'communities', 'including', 'those', 'in', 'low-income', 'countries', 'and', 'coastal', 'regions'], ['the', 'economic', 'implications', 'of', 'climate', 'change', 'are', 'significant'], ['climate', 'change', 'is', 'a', 'potential', 'driver', 'of', 'conflicts', 'over', 'resources', 'as', 'water', 'and', 'arable', 'land', 'become', 'scarcer'], ['the', 'paris', 'agreement', 'signed', 'in', '2015', 'represents', 'a', 'landmark', 'international', 'accord', 'aimed', 'at', 'addressing', 'climate', 'change'], ['transitioning', 'from', 'fossil', 'fuels', 'to', 'renewable', 'energy', 'sources', 'such', 'as', 'solar', 'wind', 'and', 'hydropower', 'is', 'a', 'crucial', 'step', 'in', 'reducing', 'greenhouse', 'gas', 'emissions'], ['reforestation', 'and', 'afforestation', 'efforts', 'along', 'with', 'sustainable', 'land', 'use', 'practices', 'can', 'help', 'sequester', 'carbon', 'and', 'protect', 'ecosystems'], ['improving', 'energy', 'efficiency', 'and', 'promoting', 'conservation', 'measures', 'can', 'reduce', 'energy', 'consumption', 'and', 'emissions'], ['raising', 'public', 'awareness', 'about', 'climate', 'change', 'is', 'essential', 'to', 'building', 'support', 'for', 'action'], ['governments', 'play', 'a', 'pivotal', 'role', 'in', 'addressing', 'climate', 'change', 'through', 'the', 'development', 'and', 'enforcement', 'of', 'policies', 'and', 'regulations'], ['investing', 'in', 'research', 'and', 'development', 'of', 'new', 'technologies', 'can', 'lead', 'to', 'breakthroughs', 'in', 'carbon', 'capture', 'renewable', 'energy', 'and', 'other', 'solutions', 'to', 'combat', 'climate', 'change'], ['climate', 'change', 'is', 'an', 'urgent', 'and', 'complex', 'issue', 'that', 'demands', 'a', 'coordinated', 'global', 'response'], ['addressing', 'climate', 'change', 'is', 'not', 'an', 'insurmountable', 'challenge'], ['the', 'urgency', 'of', 'the', 'climate', 'crisis', 'cannot', 'be', 'overstated'], ['milk', 'often', 'referred', 'to', 'as', \"nature's\", 'most', 'perfect', 'food', 'holds', 'a', 'special', 'place', 'in', 'our', 'diets', 'and', 'hearts'], ['milk', 'in', 'its', 'various', 'forms', 'is', 'a', 'nutrient', 'powerhouse'], ['dairy', 'farming', 'has', 'been', 'a', 'cornerstone', 'of', 'agriculture', 'in', 'many', 'cultures', 'around', 'the', 'world'], ['milk', 'comes', 'in', 'various', 'forms', 'each', 'tailored', 'to', 'specific', 'tastes', 'and', 'dietary', 'needs'], ['beyond', 'these', 'basic', 'forms', 'milk', 'undergoes', 'various', 'processes', 'to', 'yield', 'a', 'wide', 'array', 'of', 'dairy', 'products'], ['throughout', 'history', 'milk', 'has', 'played', 'a', 'central', 'role', 'in', 'human', 'diets', 'and', 'cultures'], [\"milk's\", 'cultural', 'significance', 'goes', 'beyond', 'its', 'culinary', 'applications'], ['the', 'health', 'benefits', 'of', 'milk', 'are', 'well-documented'], ['despite', 'its', 'many', 'virtues', \"it's\", 'essential', 'to', 'acknowledge', 'that', 'some', 'individuals', 'are', 'lactose', 'intolerant', 'or', 'have', 'dairy', 'allergies'], ['in', 'conclusion', 'milk', 'is', 'a', 'remarkable', 'and', 'multifaceted', 'elixir', 'that', 'has', 'shaped', 'our', 'diets', 'cultures', 'and', 'traditions', 'for', 'generations']]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the text corpus\n",
    "sentences = []\n",
    "with open('file.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        sentences.append(line.strip().split('.'))\n",
    "\n",
    "# Step 2: Preprocess the text corpus\n",
    "corpus=[]\n",
    "# Iterate through the list of sentences, break them into lists of words and remove the empty strings\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = sentences[i][0].split(' ')\n",
    "\n",
    "    # strip the words of any special characters\n",
    "    sentences[i] = [word.strip(',?;\"!').lower() for word in sentences[i]]\n",
    "\n",
    "    sentences[i] = list(filter(None, sentences[i]))\n",
    "    if(len(sentences[i])>0):\n",
    "        corpus.append(sentences[i])\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:10:32: collecting all words and their counts\n",
      "INFO - 18:10:32: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 18:10:32: collected 319 word types from a corpus of 558 raw words and 38 sentences\n",
      "INFO - 18:10:32: Creating a fresh vocabulary\n",
      "INFO - 18:10:32: Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 319 unique words (100.00% of original 319, drops 0)', 'datetime': '2023-10-24T18:10:32.309076', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-87-generic-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "INFO - 18:10:32: Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 558 word corpus (100.00% of original 558, drops 0)', 'datetime': '2023-10-24T18:10:32.309361', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-87-generic-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "INFO - 18:10:32: deleting the raw counts dictionary of 319 items\n",
      "INFO - 18:10:32: sample=5e-05 downsamples 319 most-common words\n",
      "INFO - 18:10:32: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 72.20881875765366 word corpus (12.9%% of prior 558)', 'datetime': '2023-10-24T18:10:32.311455', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-87-generic-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "INFO - 18:10:32: estimated required memory for 319 words and 5 dimensions: 172260 bytes\n",
      "INFO - 18:10:32: resetting layer weights\n",
      "INFO - 18:10:32: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-10-24T18:10:32.314490', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-87-generic-x86_64-with-glibc2.31', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the vocabulary\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Build the vocabulary\n",
    "print(\"Building the vocabulary\")\n",
    "w2v_model.build_vocab(corpus, progress_per=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:10:32: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 319 vocabulary and 5 features, using sg=1 hs=0 sample=5e-05 negative=5 window=3 shrink_windows=True', 'datetime': '2023-10-24T18:10:32.319902', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-87-generic-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "INFO - 18:10:32: EPOCH 0: training on 558 raw words (71 effective words) took 0.0s, 207636 effective words/s\n",
      "INFO - 18:10:32: EPOCH 1: training on 558 raw words (69 effective words) took 0.0s, 190943 effective words/s\n",
      "INFO - 18:10:32: EPOCH 2: training on 558 raw words (83 effective words) took 0.0s, 261860 effective words/s\n",
      "INFO - 18:10:32: EPOCH 3: training on 558 raw words (67 effective words) took 0.0s, 210095 effective words/s\n",
      "INFO - 18:10:32: EPOCH 4: training on 558 raw words (83 effective words) took 0.0s, 259295 effective words/s\n",
      "INFO - 18:10:32: EPOCH 5: training on 558 raw words (67 effective words) took 0.0s, 224305 effective words/s\n",
      "INFO - 18:10:32: EPOCH 6: training on 558 raw words (73 effective words) took 0.0s, 186717 effective words/s\n",
      "INFO - 18:10:32: EPOCH 7: training on 558 raw words (79 effective words) took 0.0s, 268494 effective words/s\n",
      "INFO - 18:10:32: EPOCH 8: training on 558 raw words (74 effective words) took 0.0s, 169190 effective words/s\n",
      "INFO - 18:10:32: EPOCH 9: training on 558 raw words (80 effective words) took 0.0s, 281426 effective words/s\n",
      "INFO - 18:10:32: EPOCH 10: training on 558 raw words (70 effective words) took 0.0s, 143309 effective words/s\n",
      "INFO - 18:10:32: EPOCH 11: training on 558 raw words (66 effective words) took 0.0s, 233103 effective words/s\n",
      "INFO - 18:10:32: EPOCH 12: training on 558 raw words (70 effective words) took 0.0s, 270587 effective words/s\n",
      "INFO - 18:10:32: EPOCH 13: training on 558 raw words (64 effective words) took 0.0s, 214418 effective words/s\n",
      "INFO - 18:10:32: EPOCH 14: training on 558 raw words (80 effective words) took 0.0s, 151914 effective words/s\n",
      "INFO - 18:10:32: EPOCH 15: training on 558 raw words (71 effective words) took 0.0s, 183565 effective words/s\n",
      "INFO - 18:10:32: EPOCH 16: training on 558 raw words (57 effective words) took 0.0s, 143271 effective words/s\n",
      "INFO - 18:10:32: EPOCH 17: training on 558 raw words (69 effective words) took 0.0s, 191588 effective words/s\n",
      "INFO - 18:10:32: EPOCH 18: training on 558 raw words (57 effective words) took 0.0s, 166277 effective words/s\n",
      "INFO - 18:10:32: EPOCH 19: training on 558 raw words (87 effective words) took 0.0s, 207594 effective words/s\n",
      "INFO - 18:10:32: EPOCH 20: training on 558 raw words (74 effective words) took 0.0s, 214787 effective words/s\n",
      "INFO - 18:10:32: EPOCH 21: training on 558 raw words (69 effective words) took 0.0s, 150877 effective words/s\n",
      "INFO - 18:10:32: EPOCH 22: training on 558 raw words (74 effective words) took 0.0s, 250751 effective words/s\n",
      "INFO - 18:10:32: EPOCH 23: training on 558 raw words (64 effective words) took 0.0s, 248361 effective words/s\n",
      "INFO - 18:10:32: EPOCH 24: training on 558 raw words (68 effective words) took 0.0s, 233353 effective words/s\n",
      "INFO - 18:10:32: EPOCH 25: training on 558 raw words (66 effective words) took 0.0s, 208128 effective words/s\n",
      "INFO - 18:10:32: EPOCH 26: training on 558 raw words (84 effective words) took 0.0s, 306731 effective words/s\n",
      "INFO - 18:10:32: EPOCH 27: training on 558 raw words (78 effective words) took 0.0s, 273927 effective words/s\n",
      "INFO - 18:10:32: EPOCH 28: training on 558 raw words (66 effective words) took 0.0s, 175829 effective words/s\n",
      "INFO - 18:10:32: EPOCH 29: training on 558 raw words (70 effective words) took 0.0s, 208867 effective words/s\n",
      "INFO - 18:10:32: Word2Vec lifecycle event {'msg': 'training on 16740 raw words (2150 effective words) took 0.0s, 49831 effective words/s', 'datetime': '2023-10-24T18:10:32.363838', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-87-generic-x86_64-with-glibc2.31', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2150, 16740)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Train the model\n",
    "print(\"Training the model\")\n",
    "w2v_model.train(corpus, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('overstated', 0.9642232656478882),\n",
       " ('local', 0.9492552876472473),\n",
       " ('to', 0.9347521066665649),\n",
       " ('significance', 0.9263757467269897),\n",
       " ('development', 0.9041435718536377),\n",
       " ('urgent', 0.8790487051010132),\n",
       " ('it', 0.8542263507843018),\n",
       " ('at', 0.8453495502471924),\n",
       " ('health', 0.8388596773147583),\n",
       " ('challenge', 0.827547550201416)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"climate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7292428"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('dishes', 'climate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ecosystems'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match([\"climate\", \"ecosystems\", \"dishes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
