{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalability experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepwalk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlogCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", level=logging.INFO)\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "print(cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a .mat file into a numpy array\n",
    "def load_mat(filename):\n",
    "    data = sio.loadmat(filename)\n",
    "    # return data['data']\n",
    "    return data\n",
    "    # return data['network']\n",
    "    # Extract the adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3890, 3890)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "data_np = load_mat('../datasets/ppi.mat')\n",
    "data_np = data_np['network']\n",
    "\n",
    "# Convert the sparse matrix to a dense adjacency matrix\n",
    "adjmat = data_np.toarray()\n",
    "\n",
    "print(adjmat.shape)\n",
    "print(adjmat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Ground Truth Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'blogcatalog.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/scipy/io/matlab/_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_like, mode), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     41\u001b[0m     \u001b[39m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'blogcatalog.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/anika/Desktop/smai/project-lmailmai/ppi/scalability copy.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anika/Desktop/smai/project-lmailmai/ppi/scalability%20copy.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Also obtain the labels\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/anika/Desktop/smai/project-lmailmai/ppi/scalability%20copy.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data_np \u001b[39m=\u001b[39m load_mat(\u001b[39m'\u001b[39;49m\u001b[39mblogcatalog.mat\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anika/Desktop/smai/project-lmailmai/ppi/scalability%20copy.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m labels_one_hot \u001b[39m=\u001b[39m data_np[\u001b[39m'\u001b[39m\u001b[39mgroup\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtoarray()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anika/Desktop/smai/project-lmailmai/ppi/scalability%20copy.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(labels_one_hot\u001b[39m.\u001b[39mshape)     \u001b[39m# (10312, 39)\u001b[39;00m\n",
      "\u001b[1;32m/home/anika/Desktop/smai/project-lmailmai/ppi/scalability copy.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anika/Desktop/smai/project-lmailmai/ppi/scalability%20copy.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_mat\u001b[39m(filename):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/anika/Desktop/smai/project-lmailmai/ppi/scalability%20copy.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     data \u001b[39m=\u001b[39m sio\u001b[39m.\u001b[39;49mloadmat(filename)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anika/Desktop/smai/project-lmailmai/ppi/scalability%20copy.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# return data['data']\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anika/Desktop/smai/project-lmailmai/ppi/scalability%20copy.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/scipy/io/matlab/_mio.py:225\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39mLoad MATLAB file.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m variable_names \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mvariable_names\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 225\u001b[0m \u001b[39mwith\u001b[39;49;00m _open_file_context(file_name, appendmat) \u001b[39mas\u001b[39;49;00m f:\n\u001b[1;32m    226\u001b[0m     MR, _ \u001b[39m=\u001b[39;49m mat_reader_factory(f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    227\u001b[0m     matfile_dict \u001b[39m=\u001b[39;49m MR\u001b[39m.\u001b[39;49mget_variables(variable_names)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/scipy/io/matlab/_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m@contextmanager\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     f, opened \u001b[39m=\u001b[39m _open_file(file_like, appendmat, mode)\n\u001b[1;32m     18\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[39myield\u001b[39;00m f\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/scipy/io/matlab/_mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m appendmat \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m file_like\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.mat\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m         file_like \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.mat\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_like, mode), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mReader needs file name or open file-like object\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     49\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'blogcatalog.mat'"
     ]
    }
   ],
   "source": [
    "# Also obtain the labels\n",
    "data_np = load_mat('blogcatalog.mat')\n",
    "labels_one_hot = data_np['group'].toarray()\n",
    "\n",
    "print(labels_one_hot.shape)     # (10312, 39)\n",
    "\n",
    "'''Now we know that the labels are one-hot encoded'''\n",
    "# labels = np.argmax(labels_one_hot, axis=1)\n",
    "# print(labels.shape)     # (10312,)\n",
    "# print(labels)\n",
    "\n",
    "label_sample = labels_one_hot[100]      # Thus, each node can have multiple labels (indicated by 2 ones in the array)\n",
    "print(label_sample)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the node embedding using Deepwalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepWalk():\n",
    "        def __init__(\n",
    "                        self, \n",
    "                        graph : \"list[list[int]]\", \n",
    "                        window_size : int, \n",
    "                        embedding_size : int, \n",
    "                        walks_per_vertex : int, \n",
    "                        walk_length : int\n",
    "                ) -> None:\n",
    "                \"\"\"\n",
    "                Initialize the DeepWalk model. This directly from the paper https://arxiv.org/pdf/1403.6652.pdf.\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                graph : list[list[int]]\n",
    "                        The adjacency list to be embedded. This is a list of lists, where each list is a vertex and its neighbors.\n",
    "                window_size : int\n",
    "                        The window size for the skipgram model.\n",
    "                embedding_size : int\n",
    "                        The size of the embedding. The final output matrix will be of size |V| x embedding_size.\n",
    "                walks_per_vertex : int\n",
    "                        The number of walks to perform per vertex.\n",
    "                walk_length : int\n",
    "                        The length of each walk.\n",
    "\n",
    "                Methods\n",
    "                -------\n",
    "                generate_n_walks()\n",
    "                        Generate n walks from the graph.\n",
    "                train()\n",
    "                        Train the model.\n",
    "                update()\n",
    "                        Feed model new walks.\n",
    "                get_embeddings()\n",
    "                        Return the embeddings.\n",
    "                \"\"\"\n",
    "\n",
    "                # DeepWalk parameters\n",
    "                self.g = graph\n",
    "                self.w = window_size\n",
    "                self.d = embedding_size\n",
    "                self.gamma = walks_per_vertex\n",
    "                self.epochs = self.gamma\n",
    "                self.t = walk_length\n",
    "                self.n = len(graph)\n",
    "\n",
    "        def unbiased_random_walk(\n",
    "                self,\n",
    "                adj_mat : 'list[list[int]]', \n",
    "                walk_len : 'int', \n",
    "                start_node : 'int'\n",
    "        ) -> np.array:\n",
    "                \"\"\"\n",
    "                Returns a random walk of length walk_len from start_node in the graph.\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                adj_mat : list[list[int]]\n",
    "                        Adjacency matrix of the graph.\n",
    "                walk_len : int\n",
    "                        Length of the random walk.\n",
    "                start_node : int\n",
    "                        Starting node of the random walk.\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                np.array\n",
    "                        List of nodes in the random walk.        \n",
    "                \"\"\"\n",
    "                # Array to store the walk\n",
    "                walk = [start_node]\n",
    "\n",
    "                # Make an adjacency matrix to easily \n",
    "                # sample the neighbors of a node.\n",
    "                adj_list = []\n",
    "                for i in range(len(adj_mat)):\n",
    "                        # This line was autocompleted by GH Copilot\n",
    "                        adj_list.append(np.where(adj_mat[i] > 0)[0])\n",
    "\n",
    "                # Keep walking for walk_len epochs\n",
    "                for epochs in range(walk_len):\n",
    "                        # Randomly pick a node from the adj_list[walk[-1]] \n",
    "                        # and append it to the walk\n",
    "                        walk.append(np.random.choice(adj_list[walk[-1]]))\n",
    "\n",
    "                return np.array(walk)\n",
    "\n",
    "        def generate_n_walks(self, num_iters : int) -> 'list[list[str]]':\n",
    "                \"\"\"\n",
    "                Generate a list of num_iters random walks. These will be used to train the model\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                num_iters : int\n",
    "                        Number of walks to generate.\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                np.ndarray\n",
    "                        List of random walks.\n",
    "                \"\"\"\n",
    "\n",
    "                # List to store the walks\n",
    "                walks = []\n",
    "\n",
    "                # For each vertex in the graph\n",
    "                for vertex in range(self.n):\n",
    "                        # Generate gamma walks of length t\n",
    "                        for _ in range(self.gamma):\n",
    "                                walks.append(self.unbiased_random_walk(self.g, self.t, vertex))\n",
    "\n",
    "                walks = [[str(node) for node in walk] for walk in walks]\n",
    "\n",
    "                return walks\n",
    "\n",
    "        def train(self, epochs : int, lr : float) -> None:\n",
    "                \"\"\"\n",
    "                Train the model.\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                epochs : int\n",
    "                        Number of epochs to train the model for.\n",
    "                lr : float\n",
    "                        Learning rate for the optimizer.                \n",
    "                \"\"\"\n",
    "\n",
    "                # Generate many walks\n",
    "                walks = self.generate_n_walks(self.gamma)\n",
    "\n",
    "                # Initialize the model\n",
    "                self.model = Word2Vec(\n",
    "                        walks,\n",
    "                        hs=1,\n",
    "                        sg=1,\n",
    "                        negative=0,\n",
    "                        alpha=0.05,\n",
    "                        epochs=self.epochs, \n",
    "                        vector_size=self.d,        # embedding dimension\n",
    "                        window=self.w,             # context window size\n",
    "                        min_count=0,\n",
    "                        workers=cores-2\n",
    "                )\n",
    "\n",
    "        def get_embeddings(self) -> np.ndarray:\n",
    "                \"\"\"\n",
    "                Return the embeddings.\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                np.ndarray\n",
    "                        Embeddings.\n",
    "                \"\"\"\n",
    "                return [self.model.wv[str(n)] for n in range(self.n)]\n",
    "        \n",
    "        def plot_embeddings(self, num_dimensions : int, gt_labels : 'list[str]') -> None:\n",
    "                \"\"\"\n",
    "                Plot the embeddings.\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                num_dimensions : int\n",
    "                        Number of dimensions to plot.\n",
    "                gt_labels : list[str]\n",
    "                        List of ground truth labels.\n",
    "                \"\"\"\n",
    "                embeddings = np.array(self.get_embeddings())\n",
    "\n",
    "                # dimensionality reduction to 2 dimensions using t-SNE for visualization\n",
    "                embeddings = TSNE(n_components=2).fit_transform(embeddings)\n",
    "\n",
    "                # Convert gt into numbers\n",
    "                gt_labels = np.array(gt_labels)\n",
    "\n",
    "                # Iterate over all labels and assign them an integer\n",
    "                for i, label in enumerate(np.unique(gt_labels)):\n",
    "                        gt_labels[gt_labels == label] = i\n",
    "                gt_labels = gt_labels.astype(int)\n",
    "\n",
    "                # plot the embeddings\n",
    "                plt.figure(figsize=(6,6))\n",
    "                plt.scatter(embeddings[:,0], embeddings[:,1], c=gt_labels)\n",
    "\n",
    "                # Plot node numbers\n",
    "                for i in range(len(embeddings)):\n",
    "                        plt.annotate(i , (embeddings[i,0], embeddings[i,1]))\n",
    "                        \n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjmat, window_size, embedding_size, walks_per_vertex, walk_length\n",
    "dw = DeepWalk(adjmat, 10, 128, 80, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw.train(20, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the embeddings from the model\n",
    "embeddings = np.array(dw.get_embeddings())\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a multi-label classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(range(label_sample.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a one-vs-rest logistic regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy using hamming loss and F1-score\n",
    "print(\"F1-score: \", f1_score(mlb.fit_transform(y_test),mlb.fit_transform(y_pred), average=\"micro\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
